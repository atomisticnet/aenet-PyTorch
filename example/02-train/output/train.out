======================================================================
                      Training with PyTorch-aenet                     
======================================================================


                         2023-02-10 12:56:04.                         


Developed by Jon Lopez-Zorrilla

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License in file 'LICENSE' for more details.

----------------------------------------------------------------------
                       Reading input information                      
----------------------------------------------------------------------

Reading input parameters.
These are the parameters selected for training:
        - TRAININGSET: TiO.train.ascii
        - TESTPERCENT: 0.1
        - ITERATIONS:  10000
        - ITERWRITE:   100
        - BATCHSIZE:   128
        - MEMORY_MODE: cpu

        - FORCES:      True
        - alpha:       0.1

----------------------------------------------------------------------
                   Reading training set information                   
----------------------------------------------------------------------

Training set information will be read now. If force training is
required this proccess may take some time.

The network output energy will be normalized to the interval [-1,1].
    Energy scaling factor:  f =     1.273848
    Atomic energy shift  :  s =    -7.786248


Number of structures in the data set:                1000
Number of structures with force information:          250

Atomic species in the training set: Ti  O

Average energy (eV/atom) :    -8.230179
Minimum energy (eV/atom) :    -8.571271
Maximum energy (eV/atom) :    -7.001225

----------------------------------------------------------------------
                    Preparing batches for training                    
----------------------------------------------------------------------

Batches for training are being prepared now. If force training is
required, this may take some time.

If the number of structures is not divisible by the batch size, the actual
batch size may be slightly changed.

Requested batch size: 128
Actual batch size   : 128
Number of batches   : 7

Energy batch size   : 96
Forces batch size   : 32

----------------------------------------------------------------------
                               Networks                               
----------------------------------------------------------------------

Training will be started from scratch.
Initializing networks:

Creating a network for Ti

Number of layers:    4

Number of nodes and activation type per layer:

    1 :     56 
    2 :      5   tanh
    3 :      5   tanh
    4 :      1 


Creating a network for O

Number of layers:    4

Number of nodes and activation type per layer:

    1 :     56 
    2 :      5   tanh
    3 :      5   tanh
    4 :      1 

----------------------------------------------------------------------
                           Training details                           
----------------------------------------------------------------------

Training method :        adamw
Learning rate   :     0.000100
Regularization  :     0.001000

Training device : cpu
Memory mode     : cpu

----------------------------------------------------------------------
                           Training process                           
----------------------------------------------------------------------

     epoch :  ERROR(train)   ERROR(test)   |   E (train)      E (test)   |   F (train)      F (test)
     -----    ------------   -----------        ---------      --------        ---------      --------
         0 :      0.876943      0.849528   |    0.774753      0.752434   |    1.796649      1.723380
       100 :      0.541773      0.517648   |    0.490281      0.470530   |    1.005195      0.941711
       200 :      0.271511      0.263591   |    0.203105      0.197018   |    0.887168      0.862747
       300 :      0.104080      0.117160   |    0.032285      0.046739   |    0.750230      0.750950
       400 :      0.088273      0.100974   |    0.024420      0.037437   |    0.662951      0.672802
       500 :      0.078970      0.089876   |    0.019284      0.029577   |    0.616148      0.632574
       600 :      0.072406      0.081952   |    0.015615      0.023377   |    0.583528      0.609118
       700 :      0.067711      0.076506   |    0.013112      0.019056   |    0.559102      0.593559
       800 :      0.064423      0.072778   |    0.011551      0.016242   |    0.540262      0.581603
       900 :      0.062061      0.070081   |    0.010565      0.014322   |    0.525529      0.571918
      1000 :      0.060277      0.068007   |    0.009886      0.012930   |    0.513795      0.563694
      1100 :      0.058858      0.066342   |    0.009375      0.011899   |    0.504211      0.556322
      1200 :      0.057681      0.064962   |    0.008962      0.011128   |    0.496155      0.549465
      1300 :      0.056670      0.063786   |    0.008614      0.010548   |    0.489178      0.542921
      1400 :      0.055779      0.062754   |    0.008315      0.010111   |    0.482953      0.536540
      1500 :      0.054976      0.061827   |    0.008057      0.009783   |    0.477245      0.530227
      1600 :      0.054240      0.060975   |    0.007835      0.009533   |    0.471885      0.523949
      1700 :      0.053557      0.060178   |    0.007646      0.009340   |    0.466752      0.517721
      1800 :      0.052915      0.059424   |    0.007487      0.009185   |    0.461763      0.511576
      1900 :      0.052307      0.058706   |    0.007355      0.009056   |    0.456881      0.505555
      2000 :      0.051731      0.058026   |    0.007244      0.008950   |    0.452108      0.499707
      2100 :      0.051185      0.057390   |    0.007152      0.008868   |    0.447478      0.494092
      2200 :      0.050669      0.056805   |    0.007074      0.008809   |    0.443031      0.488769
      2300 :      0.050184      0.056273   |    0.007005      0.008772   |    0.438791      0.483782
      2400 :      0.049726      0.055792   |    0.006943      0.008752   |    0.434770      0.479154
      2500 :      0.049294      0.055360   |    0.006887      0.008746   |    0.430962      0.474884
      2600 :      0.048886      0.054970   |    0.006834      0.008749   |    0.427357      0.470959
      2700 :      0.048501      0.054619   |    0.006785      0.008760   |    0.423940      0.467353
      2800 :      0.048135      0.054301   |    0.006740      0.008775   |    0.420699      0.464035
      2900 :      0.047788      0.054011   |    0.006696      0.008793   |    0.417620      0.460968
      3000 :      0.047458      0.053745   |    0.006655      0.008814   |    0.414692      0.458117
      3100 :      0.047143      0.053498   |    0.006614      0.008837   |    0.411904      0.455449
      3200 :      0.046843      0.053268   |    0.006575      0.008860   |    0.409248      0.452932
      3300 :      0.046555      0.053050   |    0.006537      0.008883   |    0.406715      0.450545
      3400 :      0.046279      0.052841   |    0.006499      0.008905   |    0.404298      0.448266
      3500 :      0.046014      0.052641   |    0.006461      0.008925   |    0.401991      0.446082
      3600 :      0.045759      0.052446   |    0.006422      0.008943   |    0.399788      0.443982
      3700 :      0.045514      0.052257   |    0.006384      0.008957   |    0.397684      0.441956
      3800 :      0.045279      0.052070   |    0.006346      0.008967   |    0.395673      0.440000
      3900 :      0.045052      0.051887   |    0.006308      0.008973   |    0.393750      0.438108
      4000 :      0.044833      0.051706   |    0.006269      0.008976   |    0.391910      0.436278
      4100 :      0.044623      0.051527   |    0.006231      0.008974   |    0.390147      0.434506
      4200 :      0.044420      0.051351   |    0.006194      0.008969   |    0.388457      0.432790
      4300 :      0.044224      0.051177   |    0.006156      0.008960   |    0.386834      0.431130
      4400 :      0.044035      0.051006   |    0.006119      0.008949   |    0.385274      0.429523
      4500 :      0.043852      0.050838   |    0.006083      0.008934   |    0.383773      0.427968
      4600 :      0.043675      0.050672   |    0.006047      0.008918   |    0.382326      0.426463
      4700 :      0.043503      0.050510   |    0.006012      0.008899   |    0.380931      0.425008
      4800 :      0.043338      0.050352   |    0.005977      0.008880   |    0.379583      0.423600
      4900 :      0.043177      0.050197   |    0.005943      0.008859   |    0.378280      0.422239
      5000 :      0.043021      0.050046   |    0.005910      0.008838   |    0.377019      0.420922
      5100 :      0.042870      0.049899   |    0.005878      0.008816   |    0.375799      0.419648
      5200 :      0.042723      0.049757   |    0.005846      0.008795   |    0.374616      0.418416
      5300 :      0.042581      0.049618   |    0.005815      0.008773   |    0.373470      0.417223
      5400 :      0.042442      0.049484   |    0.005785      0.008752   |    0.372358      0.416069
      5500 :      0.042308      0.049353   |    0.005756      0.008731   |    0.371280      0.414951
      5600 :      0.042178      0.049226   |    0.005727      0.008710   |    0.370234      0.413868
      5700 :      0.042051      0.049103   |    0.005699      0.008690   |    0.369218      0.412818
      5800 :      0.041928      0.048983   |    0.005672      0.008669   |    0.368233      0.411801
      5900 :      0.041809      0.048866   |    0.005646      0.008649   |    0.367276      0.410815
      6000 :      0.041694      0.048752   |    0.005621      0.008629   |    0.366347      0.409858
      6100 :      0.041581      0.048640   |    0.005596      0.008608   |    0.365444      0.408930
      6200 :      0.041472      0.048532   |    0.005573      0.008588   |    0.364568      0.408029
      6300 :      0.041366      0.048425   |    0.005550      0.008566   |    0.363716      0.407155
      6400 :      0.041264      0.048321   |    0.005528      0.008545   |    0.362888      0.406306
      6500 :      0.041164      0.048218   |    0.005506      0.008522   |    0.362083      0.405482
      6600 :      0.041067      0.048118   |    0.005486      0.008500   |    0.361300      0.404682
      6700 :      0.040973      0.048019   |    0.005466      0.008476   |    0.360538      0.403906
      6800 :      0.040882      0.047922   |    0.005447      0.008452   |    0.359796      0.403153
      6900 :      0.040793      0.047827   |    0.005428      0.008428   |    0.359073      0.402422
      7000 :      0.040706      0.047734   |    0.005411      0.008403   |    0.358369      0.401713
      7100 :      0.040622      0.047643   |    0.005393      0.008378   |    0.357682      0.401025
      7200 :      0.040540      0.047553   |    0.005377      0.008353   |    0.357013      0.400358
      7300 :      0.040461      0.047466   |    0.005361      0.008327   |    0.356359      0.399712
      7400 :      0.040383      0.047380   |    0.005345      0.008301   |    0.355720      0.399085
      7500 :      0.040307      0.047296   |    0.005330      0.008276   |    0.355097      0.398477
      7600 :      0.040233      0.047214   |    0.005316      0.008250   |    0.354487      0.397888
      7700 :      0.040161      0.047134   |    0.005302      0.008225   |    0.353890      0.397317
      7800 :      0.040090      0.047056   |    0.005289      0.008200   |    0.353307      0.396763
      7900 :      0.040022      0.046980   |    0.005276      0.008175   |    0.352736      0.396227
      8000 :      0.039954      0.046906   |    0.005263      0.008150   |    0.352177      0.395707
      8100 :      0.039888      0.046834   |    0.005250      0.008126   |    0.351629      0.395202
      8200 :      0.039824      0.046763   |    0.005239      0.008102   |    0.351091      0.394713
      8300 :      0.039761      0.046695   |    0.005227      0.008079   |    0.350565      0.394238
      8400 :      0.039699      0.046629   |    0.005216      0.008056   |    0.350048      0.393778
      8500 :      0.039638      0.046564   |    0.005205      0.008034   |    0.349541      0.393331
      8600 :      0.039579      0.046501   |    0.005194      0.008012   |    0.349044      0.392897
      8700 :      0.039521      0.046439   |    0.005183      0.007991   |    0.348555      0.392475
      8800 :      0.039463      0.046379   |    0.005173      0.007970   |    0.348075      0.392065
      8900 :      0.039407      0.046321   |    0.005163      0.007949   |    0.347603      0.391667
      9000 :      0.039352      0.046264   |    0.005154      0.007929   |    0.347139      0.391280
      9100 :      0.039298      0.046209   |    0.005144      0.007909   |    0.346683      0.390903
      9200 :      0.039245      0.046154   |    0.005135      0.007890   |    0.346234      0.390536
      9300 :      0.039193      0.046102   |    0.005126      0.007871   |    0.345793      0.390180
      9400 :      0.039141      0.046050   |    0.005117      0.007852   |    0.345358      0.389832
      9500 :      0.039090      0.045999   |    0.005108      0.007833   |    0.344930      0.389494
      9600 :      0.039041      0.045950   |    0.005100      0.007815   |    0.344508      0.389164
      9700 :      0.038992      0.045901   |    0.005092      0.007797   |    0.344092      0.388843
      9800 :      0.038943      0.045854   |    0.005083      0.007779   |    0.343682      0.388529
      9900 :      0.038896      0.045807   |    0.005075      0.007761   |    0.343278      0.388224

Time needed for training:         3190.566039 s
Maximum CPU memory used:             2.108723 GB
Maximum GPU memory used:             0.000000 GB


----------------------------------------------------------------------
                            Storing results                           
----------------------------------------------------------------------

saving train energy error to : energy.train
saving test energy error to  : energy.test

Saving the Ti network to file : Ti.pytorch.nn.ascii
Saving the O network to file : O.pytorch.nn.ascii


                         2023-02-10 13:49:27.                         


======================================================================
                     Neural Network training done.                    
======================================================================
